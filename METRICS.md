# Metrics Reference & Chart Guide

This document describes each metric captured by AuditAI, including formulas, goals, and example values, as well as the charts used to visualize them in the Streamlit dashboard.

---

## Observability Pipeline Overview

1. **Request Interception**
   - FastAPI `AuditMiddleware` captures each inference call: records timestamps, parses request bodies, and reads response payloads.
   - Converts raw HTTP data into structured inputs (`prompt`, `model`) and outputs (`completion`, `status`).

2. **Metric Instrumentation**
   - Middleware invokes `StandardMetricsTracker.log_inference(...)` with prompt, completion, timing, and status.
   - Token counts (`tokens_in`, `tokens_out`) computed via real tokenizer on prompt/completion texts.
   - Process‐level resource metrics captured **per inference** via `psutil.Process`:
     - **cpu_time_sec**: CPU time (user+system) spent in seconds during the call
     - **memory_delta_bytes**: Increase in RSS memory (bytes) during the call
     - **disk_percent**: Disk usage percent at call end (optional)

3. **Quality Detection Modules**
   - **HallucinationDetector** computes a weighted score combining:
     - Semantic divergence (cosine distance of embeddings via `all-MiniLM-L6-v2`). It's the most important factor in the formula.
     - Heuristic hesitation phrases.
     - Keyword overlap ratio.
  
     #### Hallucination Detection Approach
     - Normalizes prompt and completion text to lower case.
     - Computes three sub‐scores:
       - **Hesitation Score**: tallies occurrences of uncertainty indicators (e.g., “I’m not sure”, “perhaps”), capped at 0.5.
       - **Overlap Score**: measures inverse overlap ratio between prompt and response word sets (more overlap → lower hallucination).
       - **Semantic Divergence**: 1 − cosine similarity of embeddings generated by `sentence-transformers/all-MiniLM-L6-v2`.
     - Forms final score with a weighted sum: `0.6*semantic_divergence + 0.2*hesitation_score + 0.2*overlap_score`, then clamps to [0,1].
   

   - **DriftDetector** maintains a rolling window of recent scores and compares against reference statistics to flag distributional changes.
  
     #### Drift Detection Approach
     - Maintains a sliding window (`window_size`, default 10) of the most recent inference metrics.
     - On each call, appends current `hallucination_score`, `response_time_ms` (and optionally `token_count`) to the window, dropping the oldest when full.
     - Computes window means: `mean(hallucination_score)` and `mean(response_time_ms)`.
     - Loads baseline/reference means from a precomputed JSON (if provided) under keys `hallucination_score_mean` and `response_time_ms_mean`.
     - Calculates relative change:
       - Hallucination drift: `abs(current_mean - reference_mean) / reference_mean > 0.2`
       - Response time drift: `(current_mean - reference_mean) / reference_mean > 0.5` (only positive drift)
     - Flags drift when either condition is met, returning a boolean per inference.
  

4. **Data Persistence**
   - Each inference record is serialized as JSONL under `data/metrics/metrics.jsonl`.
   - On startup, `StandardMetricsTracker` loads historical data into a Pandas DataFrame for aggregation.

5. **Visualization**
   - The Streamlit dashboard reads the DataFrame and computes derived fields (e.g., `token_efficiency`, `response_quality_index`, `hour`, `day_of_week`).
   - Charts are rendered using Plotly Express and Plotly Graph Objects to display KPIs, trends, correlations, and advanced analytics.

---

## Metrics Schema

1. **response_time_ms**  
   - Formula: `(end_timestamp - start_timestamp) * 1000`  
   - Goal: Measure end-to-end latency for each inference call (ms).  
   - Example: 1301

2. **tokens_in**  
   - Formula: `len(tokenizer.encode(prompt, add_special_tokens=False))` with `e5-multilingual` tokenizer.  
  
   - Goal: Count input tokens to estimate model cost and throughput.  
   - Example: 34

3. **tokens_out**  
   - Formula: `len(tokenizer.encode(completion, add_special_tokens=False))` with `e5-multilingual` tokenizer.  
   - Goal: Count output tokens for billing and efficiency analysis.  
   - Example: 45

4. **hallucination_score**  
   - Formula: `(semantic_divergence * 0.6) + (hesitation_score * 0.2) + (overlap_score * 0.2)`, range 0–1.  
   - Goal: Quantify probability of content hallucination.  
   - Example: 0.336

5. **fact_consistency**  
   - From `HallucinationDetector.get_fact_consistency(...)`, range 0–1.  
   - Goal: Measure factual alignment when hallucination score < 0.8.  
   - Example: 0.664

6. **drift_detected**  
   - Boolean: `True` if `DriftDetector.detect_drift(...)` flags distributional change.  
   - Goal: Alert on input/output distribution drift over time windows.  
   - Example: `false`

7. **cpu_time_sec**  
   - Formula: `(end_cpu.user+end_cpu.system) - (start_cpu.user+start_cpu.system)`  
   - Goal: Measure CPU seconds consumed by the inference process, isolating the model call cost.  
   - Example: 0.023

8. **memory_delta_bytes**  
   - Formula: `end_mem.rss - start_mem.rss`  
   - Goal: Track per‐inference increase in process memory usage (RSS), in bytes.  
   - Example: `1048576` (1 MB)

9. **disk_percent**  
   - From `psutil.disk_usage('/').percent` (captured at end of inference)  
   - Goal: Monitor disk utilization anomalies if any.  
   - Example: 55.3

---

## Derived Metrics

1. **token_efficiency**  
   - Formula: `tokens_out / max(tokens_in, 1)`  
   - Goal: Evaluate generation efficiency (ratio of output to input tokens).  
   - Example: `45 / 34 ≈ 1.32`

2. **response_quality_index**  
   - Formula: `0.4*(1 - hallucination_score) + 0.4*fact_consistency + 0.2*(1 - (response_time_ms / max_response_time))`  
   - Goal: Aggregate quality signals (accuracy & performance) into a single index [0–1].  
   - Example: `0.4*(1-0.336) + 0.4*0.664 + 0.2*(1-1301/2000) ≈ 0.75`

3. **hallucination_category**  
   - Bins on `hallucination_score`:  
     - 0–0.3 = Low  
     - 0.3–0.5 = Medium  
     - 0.5–1.0 = High  
   - Goal: Classify severity for easy filtering.

4. **hour**, **day_of_week**  
   - Extracted from timestamp:  
     - `hour = timestamp.hour`  
     - `day_of_week = timestamp.dayofweek`  
   - Goal: Enable temporal analysis (e.g., peak usage hours).

---

## Dashboard Charts

See the `CHARTS.md` file for more details.


> **Additional Informations**  
> All metrics and thresholds are **experimental**. They can be tuned, extended, or replaced with more advanced statistical methods.
